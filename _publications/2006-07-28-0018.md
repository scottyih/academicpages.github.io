---
title: "Learning at Low False Positive Rates"
collection: publications
permalink: /publication/2006-07-28-0018
abstract: 'Most spam filters are configured for use at a very low falsepositive
rate. Typically, the filters are trained with techniques
that optimize accuracy or entropy, rather than performance
in this configuration. We describe two different
techniques for optimizing for the low false-positive region.
One method weights good data more than spam. The other
method uses a two-stage technique of first finding data in the
low false-positive region, and then learning using this subset.
We show that with two different learning algorithms,
logistic regression and Naive Bayes, we achieve substantial
improvements, reducing missed spam by as much as 20%
relative for logistic regression and 40% for Naive Bayes at
the same low false-positive rate.'
date: 2006-07-28
author: 'W. Yih, J. Goodman and G. Hulten'
venue: 'CEAS-2006'
paperurl: '../files/YihGoodmanHulten-ceas06.pdf'
biburl: '../publications/2006-07-28-0018.txt'
slides_poster: https://github.com/scottyih/Slides/blob/master/ceas06%20-%20Deck.pptx
---

<a href='../files/YihGoodmanHulten-ceas06.pdf'>Download paper here</a>

Most spam filters are configured for use at a very low falsepositive
rate. Typically, the filters are trained with techniques
that optimize accuracy or entropy, rather than performance
in this configuration. We describe two different
techniques for optimizing for the low false-positive region.
One method weights good data more than spam. The other
method uses a two-stage technique of first finding data in the
low false-positive region, and then learning using this subset.
We show that with two different learning algorithms,
logistic regression and Naive Bayes, we achieve substantial
improvements, reducing missed spam by as much as 20%
relative for logistic regression and 40% for Naive Bayes at
the same low false-positive rate.
