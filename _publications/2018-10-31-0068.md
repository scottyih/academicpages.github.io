---
title: "Dissecting Contextual Word Embeddings: Architecture and Representation"
collection: publications
permalink: /publication/2018-10-31-0068
abstract: 'Contextual word representations derived from
pre-trained bidirectional language models
(biLMs) have recently been shown to provide
significant improvements to the state of the
art for a wide range of NLP tasks. However,
many questions remain as to how and
why these models are so effective. In this
paper, we present a detailed empirical study
of how the choice of neural architecture (e.g.
LSTM, CNN, or self attention) influences both
end task accuracy and qualitative properties of
the representations that are learned. We show
there is a tradeoff between speed and accuracy,
but all architectures learn high quality contextual
representations that outperform word
embeddings for four challenging NLP tasks.
Additionally, all architectures learn representations
that vary with network depth, from exclusively
morphological based at the word embedding
layer through local syntax based in the
lower contextual layers to longer range semantics
such coreference at the upper layers. Together,
these results suggest that unsupervised
biLMs, independent of architecture, are learning
much more about the structure of language
than previously appreciated.'
date: 2018-10-31
author: 'Matthew Peters, Mark Neumann, Wen-tau Yih and Luke Zettlemoyer'
venue: 'EMNLP-2018'
paperurl: '../files/1808.08949.pdf'
biburl: '../publications/2018-10-31-0068.txt'
---

<a href='../files/1808.08949.pdf'>Download paper here</a>

Contextual word representations derived from
pre-trained bidirectional language models
(biLMs) have recently been shown to provide
significant improvements to the state of the
art for a wide range of NLP tasks. However,
many questions remain as to how and
why these models are so effective. In this
paper, we present a detailed empirical study
of how the choice of neural architecture (e.g.
LSTM, CNN, or self attention) influences both
end task accuracy and qualitative properties of
the representations that are learned. We show
there is a tradeoff between speed and accuracy,
but all architectures learn high quality contextual
representations that outperform word
embeddings for four challenging NLP tasks.
Additionally, all architectures learn representations
that vary with network depth, from exclusively
morphological based at the word embedding
layer through local syntax based in the
lower contextual layers to longer range semantics
such coreference at the upper layers. Together,
these results suggest that unsupervised
biLMs, independent of architecture, are learning
much more about the structure of language
than previously appreciated.
