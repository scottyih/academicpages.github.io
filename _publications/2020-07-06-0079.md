---
title: "TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data"
collection: publications
permalink: /publication/2020-07-06-0079
abstract: 'Recent years have witnessed the burgeoning of pretrained language models (LMs) for text-based natural language (NL) understanding tasks. Such models are typically trained on free-form NL text, hence may not be suitable for tasks like semantic parsing over structured data, which require reasoning over both free-form NL questions and structured tabular data (e.g., database tables). In this paper we present TaBERT, a pretrained LM that jointly learns representations for NL sentences and (semi-)structured tables. TaBERT is trained on a large corpus of 26 million tables and their English contexts. In experiments, neural semantic parsers using TaBERT as feature representation layers achieve new best results on the challenging weakly-supervised semantic parsing benchmark WikiTableQuestions, while performing competitively on the text-to-SQL dataset Spider.'
date: 2020-07-06
author: 'Pengcheng Yin, Graham Neubig, Wen-tau Yih, Sebastian Riedel'
venue: 'ACL-2020'
paperurl: '../files/2020.acl-main.745.pdf'
biburl: '../publications/2020-07-06-0079.txt'
video: http://slideslive.com/38929345
code: https://github.com/facebookresearch/TaBERT
---

<a href='../files/2020.acl-main.745.pdf'>Download paper here</a>

Recent years have witnessed the burgeoning of pretrained language models (LMs) for text-based natural language (NL) understanding tasks. Such models are typically trained on free-form NL text, hence may not be suitable for tasks like semantic parsing over structured data, which require reasoning over both free-form NL questions and structured tabular data (e.g., database tables). In this paper we present TaBERT, a pretrained LM that jointly learns representations for NL sentences and (semi-)structured tables. TaBERT is trained on a large corpus of 26 million tables and their English contexts. In experiments, neural semantic parsers using TaBERT as feature representation layers achieve new best results on the challenging weakly-supervised semantic parsing benchmark WikiTableQuestions, while performing competitively on the text-to-SQL dataset Spider.
