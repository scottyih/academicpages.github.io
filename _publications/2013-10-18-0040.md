---
title: "Dual Coordinate Descent Algorithms for Efficient Large Margin Structured Prediction"
collection: publications
permalink: /publication/2013-10-18-0040
abstract: 'Due to the nature of complex NLP problems,
structured prediction algorithms have been
important modeling tools for a wide range of
tasks. While there exists evidence showing
that linear Structural Support Vector Machine
(SSVM) algorithm performs better than structured
Perceptron, the SSVM algorithm is still
less frequently chosen in the NLP community
because of its relatively slow training speed.
In this paper, we propose a fast and easy-toimplement
dual coordinate descent algorithm
for SSVMs. Unlike algorithms such as Perceptron
and stochastic gradient descent, our
method keeps track of dual variables and updates
the weight vector more aggressively. As
a result, this training process is as efficient as
existing online learning methods, and yet derives
consistently better models, as evaluated
on four benchmark NLP datasets for part-ofspeech
tagging, named-entity recognition and
dependency parsing.'
date: 2013-10-18
author: 'Ming-Wei Chang and Wen-tau Yih'
venue: 'TACL'
paperurl: '../files/Q13-1017.pdf'
biburl: '../publications/2013-10-18-0040.txt'
slides_poster: https://github.com/scottyih/Slides/blob/master/ChangYih13_slide.pptx
---

<a href='../files/Q13-1017.pdf'>Download paper here</a>

Due to the nature of complex NLP problems,
structured prediction algorithms have been
important modeling tools for a wide range of
tasks. While there exists evidence showing
that linear Structural Support Vector Machine
(SSVM) algorithm performs better than structured
Perceptron, the SSVM algorithm is still
less frequently chosen in the NLP community
because of its relatively slow training speed.
In this paper, we propose a fast and easy-toimplement
dual coordinate descent algorithm
for SSVMs. Unlike algorithms such as Perceptron
and stochastic gradient descent, our
method keeps track of dual variables and updates
the weight vector more aggressively. As
a result, this training process is as efficient as
existing online learning methods, and yet derives
consistently better models, as evaluated
on four benchmark NLP datasets for part-ofspeech
tagging, named-entity recognition and
dependency parsing.
