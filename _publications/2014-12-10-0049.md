---
title: "Joint Semantic Utterance Classification and Slot Filling with Recursive Neural Networks"
collection: publications
permalink: /publication/2014-12-10-0049
abstract: 'In recent years, continuous space models have proven to be
highly effective at language processing tasks ranging from
paraphrase detection to language modeling. These models
are distinctive in their ability to achieve generalization
through continuous space representations, and compositionality
through arithmetic operations on those representations.
Examples of such models include feed-forward and recurrent
neural network language models. Recursive neural networks
(RecNNs) extend this framework by providing an
elegant mechanism for incorporating both discrete syntactic
structure and continuous-space word and phrase representations
into a powerful compositional model. In this paper,
we show that RecNNs can be used to perform the core spoken
language understanding (SLU) tasks in a spoken dialog
system, more specifically domain and intent determination,
concurrently with slot filling, in one jointly trained model.
We find that a very simple RecNN model achieves competitive
performance on the benchmark ATIS task, as well as on
a Microsoft Cortana conversational understanding task.'
date: 2014-12-10
author: 'Daniel Guo, Gokhan Tur, Wen-tau Yih and Geoffrey Zweig'
venue: 'SLT-2014'
paperurl: '../files/RecNNSLU.pdf'
biburl: '../publications/2014-12-10-0049.txt'
---

<a href='../files/RecNNSLU.pdf'>Download paper here</a>

In recent years, continuous space models have proven to be
highly effective at language processing tasks ranging from
paraphrase detection to language modeling. These models
are distinctive in their ability to achieve generalization
through continuous space representations, and compositionality
through arithmetic operations on those representations.
Examples of such models include feed-forward and recurrent
neural network language models. Recursive neural networks
(RecNNs) extend this framework by providing an
elegant mechanism for incorporating both discrete syntactic
structure and continuous-space word and phrase representations
into a powerful compositional model. In this paper,
we show that RecNNs can be used to perform the core spoken
language understanding (SLU) tasks in a spoken dialog
system, more specifically domain and intent determination,
concurrently with slot filling, in one jointly trained model.
We find that a very simple RecNN model achieves competitive
performance on the benchmark ATIS task, as well as on
a Microsoft Cortana conversational understanding task.
