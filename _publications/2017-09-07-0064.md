---
title: "Maximum margin reward networks for learning from explicit and implicit supervision"
collection: publications
permalink: /publication/2017-09-07-0064
abstract: 'Neural networks have achieved state-of-the-art performance on several
structured-output prediction tasks, trained in a fully supervised
fashion.  However, annotated examples in structured domains are often
costly to obtain, which thus limits the applications of neural
networks.  In this work, we propose Maximum Margin Reward Networks, a
neural network-based framework that aims to learn from both explicit
(full structures) and implicit supervision signals (delayed feedback
on the correctness of the predicted structure).  On named entity
recognition and semantic parsing, our model outperforms previous
systems on the benchmark datasets, CoNLL-2003 and WebQuestionsSP.'
date: 2017-09-07
author: 'Haoruo Peng, Ming-Wei Chang and Wen-tau Yih'
venue: 'EMNLP-2017'
paperurl: '../files/D17-1252.pdf'
biburl: '../publications/2017-09-07-0064.txt'
video: https://vimeo.com/238234174
---

<a href='../files/D17-1252.pdf'>Download paper here</a>

Neural networks have achieved state-of-the-art performance on several
structured-output prediction tasks, trained in a fully supervised
fashion.  However, annotated examples in structured domains are often
costly to obtain, which thus limits the applications of neural
networks.  In this work, we propose Maximum Margin Reward Networks, a
neural network-based framework that aims to learn from both explicit
(full structures) and implicit supervision signals (delayed feedback
on the correctness of the predicted structure).  On named entity
recognition and semantic parsing, our model outperforms previous
systems on the benchmark datasets, CoNLL-2003 and WebQuestionsSP.
